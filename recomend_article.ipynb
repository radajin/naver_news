{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import konlpy.tag \n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Article & Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_df(df):\n",
    "    return df.iloc[np.random.permutation(len(df))].reset_index(drop=True) \n",
    "\n",
    "def get_datas(date, comment=1):\n",
    "    article_df = pd.read_csv(\"../data/news/\" + date + \".csv\")\n",
    "    article_df = shuffle_df(article_df)\n",
    "    comment_df = pd.read_csv(\"../data/comment/comment\"+str(comment)+\".csv\")\n",
    "    return article_df, comment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Morpheme Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_document_morpheme(texts):\n",
    "    \"\"\"make documents list after morphological\"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    komoran = konlpy.tag.Komoran()\n",
    "    \n",
    "    for text in texts:\n",
    "        obj = line2obj(komoran.pos(text, flatten=False))\n",
    "        documents.append(obj)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "def line2obj(lines):\n",
    "    \"\"\" make keywords dictionary include only (NNP, NNG)\"\"\"\n",
    "    \n",
    "    obj = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        for keyword in line:\n",
    "            if len(keyword[0]) > 1 and ( keyword[1] == \"NNP\" or keyword[1] == \"NNG\" ) :\n",
    "                key = keyword[0]\n",
    "                if key in obj:\n",
    "                    obj[key] += 1\n",
    "                else:\n",
    "                    obj[key] = 1\n",
    "                    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_text_documents(documents):\n",
    "    \n",
    "    text_documents = []\n",
    "    \n",
    "    for document in documents:\n",
    "        \n",
    "        text_words = []\n",
    "        \n",
    "        for word, count in document.items():\n",
    "            text_words.extend([word] * count)\n",
    "            \n",
    "        text_document = \" \".join(text_words)\n",
    "        text_documents.append(text_document)\n",
    "        \n",
    "    return text_documents        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_euc_dist(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    dist = sp.linalg.norm(delta.toarray())\n",
    "    return dist\n",
    "\n",
    "def calc_cos_dist(v1, v2):\n",
    "#     v1 = v1.toarray()\n",
    "#     v2 = v2.toarray()\n",
    "#     dist = 1.0 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    dist = spatial.distance.cosine(v1.toarray(), v2.toarray())\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def get_euc_dists(vectorized, num_docs):\n",
    "    \n",
    "    dists = []\n",
    "    \n",
    "    for idx in range(num_docs-1):\n",
    "#         dist = calc_euc_dist(vectorized.getrow(num_docs-1), vectorized.getrow(idx))\n",
    "        dist = calc_cos_dist(vectorized.getrow(num_docs-1), vectorized.getrow(idx))\n",
    "        dists.append((num_docs-1, idx, dist))\n",
    "        \n",
    "    return dists\n",
    "\n",
    "def sort_dists(dists):\n",
    "    result_list = []\n",
    "    for i, j, dist in sorted(dists, key=operator.itemgetter(2)):\n",
    "        result_list.append((i, j, dist))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2241, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "article_df, comment_df = get_datas(\"2016-07-06\", 1)\n",
    "len(article_df), len(comment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.2 s, sys: 449 ms, total: 36.7 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "# make documents\n",
    "comment_str = comment_df[\"content\"][:].str.cat(sep=' ')\n",
    "article_series = article_df[\"content\"][:2000]\n",
    "article_series = article_series.append(pd.Series([comment_str])).reset_index(drop=True)\n",
    "%time article_documents = make_document_morpheme(article_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make gen documents\n",
    "gen_articles = gen_text_documents(article_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of input documents : 2001\n",
      "the number of words : 22958\n"
     ]
    }
   ],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = HashingVectorizer()\n",
    "\n",
    "vectorized = vectorizer.fit_transform(gen_articles)\n",
    "num_docs, num_features =  vectorized.shape\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"the number of input documents : {}\".format(num_docs))\n",
    "print(\"the number of words : {}\".format(num_features))\n",
    "result_df = pd.DataFrame(vectorized.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Measurement Distance between comment and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2000, 1912, 0.86779432516931476),\n",
       " (2000, 1658, 0.86837712806770639),\n",
       " (2000, 1550, 0.87407558097722782),\n",
       " (2000, 217, 0.8861027940442362),\n",
       " (2000, 671, 0.88651258206522221),\n",
       " (2000, 1163, 0.88861011320450345),\n",
       " (2000, 1039, 0.90234043545470832),\n",
       " (2000, 1127, 0.91141615323066449),\n",
       " (2000, 998, 0.91672520814115388),\n",
       " (2000, 636, 0.91992304322809038)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = get_euc_dists(vectorized, num_docs)\n",
    "result = sort_dists(dists)\n",
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화 `로봇소리`많이 기대하고 있습니다. 폭풍 연기력을 보여주실 이성민 배우분의 연기력을 기대해봅니다. 우선 오늘 저녁8부터는 다시 항공편 운항 재개를 한다고 하니 조금만 더 기다리시면 될 것 같아요. 제주도 관계자분들도 힘내세요! 영화 `로봇소리`많이 기대하고 있습니다. 폭풍 연기력을 보여주실 이성민 배우분의 연기력을 기대해봅니다. 아울러 대구 지하철 참사를 다시 생각하게 해주셔서 감사합니다. 제주도를 비롯해서 다른 지역 충청이남서해안 지역분들과 전라도 부근 주민들도 힘내세요! 눈 때문에 고생이 많으세요! 그래도 자수를 했다니 다행이네요, 피해자분들도 마음 잘 추수리셔서 화이팅하셨으면 좋겠습니다. 경찰분들도 바쁘신건 알지만 힘 내주세요! 충북 제천지역도 다른지역과 같이 많이 추운가 봅니다ㅠ 농민분들이 엄청 고생하시는 것 같습니다. 조금만 있으면 날씨가 풀릴 것 같으니 힘내세요! 평창올림픽의 성공을 위하여 관련 관계자분들이 스페인 마드리드까지 가서 신다고 하는데 감사합니다. 평창올림픽의 성공적인 개최를 기원합니다. KTX 지연, 제주공항 운항 지연에 뒤이은 여러번의 동파사고까지, 조금만 지나면 날시가 풀릴테니 힘내세요!! 모쪼록 몸에 해로운 담배입니다. 다른 나라의 광고가 어떻든 대한민국 국민의 건강 증진을 위하여 금연을 기원하시는 분들은 금연에 성공하시길 바라겠습니다. 어린 아이들이 이렇게 모범을 보여주니 너무 고맙습니다. 화단정리에 노인들께 식사를 대접하기까지 너무 마음이 예쁩니다. 이렇게 노인장기요양서비스가 통합(통일)된다면 노인분들이 이용하시기에도 간편하고 좋은 서비스활동이 될 것 같습니다. 미래를 위해 방학중에 대학 공동 진로진학 박람회까지 가서 자신의 미래를 설꼐하는것을 보면 너무 아름다운 것 같습니다. 이 추운 날씨에도 고생해주시는 장병분들을 보면 감사한 마음 뿐입니다. 급식을 비롯해서 여러 여건들이 개선되길 희망합니다! 이렇게 추운날씨에 남들을 위해  쌀 1000kg을 기부해주신 대한불교조계종 문수사(주지 스님 혜안)께 너무 감사합니다! 이렇게 중소기업과 소상공인을 위한 정책들이 많이 생겨서 모두가 행복한 대한민국이 되면 좋겠습니다. 관련 행정적 지원을 해주신 구청장님 감사합니다!\n"
     ]
    }
   ],
   "source": [
    "# comment\n",
    "print(comment_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recomend Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1912\n",
      "[글로벌 기업들 로봇 경쟁] 구글 - 알파고와 같은 인공지능으로 작동하는 정교한 로봇팔 훈련 중도요타 - 2020년 도우미 로봇 출시 계획현대차 - 웨어러블 로봇 시제품 공개LG전자 - 인천공항 안내 로봇 곧 선보여글로벌 기업들의 로봇 기술 경쟁이 뜨거워지고 있다. 지금까지는 생산 현장에서 단순 작업을 반복하는 공정 자동화용 로봇을 주로 활용해왔지만 최근에는 인공지능을 심은 서비스 로봇이 주목받고 있다. 시장이 본격적 성장 조짐을 보이면서 전자·소프트웨어는 물론 자동차 분야까지 세계적 기업이 잇따라 뛰어들기 시작했다. KAIST 오준호 교수(기계공학)는 \"현재 로봇은 산업용 이외에 군사용 등 특수 목적으로만 일부 쓰이고 있지만 인공지능과 결합하면 다양한 서비스 로봇이 등장할 것\"이라고 말했다.◇몸체 갖춘 '알파고'의 등장세계 최대 인터넷 기업 구글은 바둑 프로그램 '알파고'처럼 인공지능으로 작동하는 로봇을 개발 중이다. 알파고는 소프트웨어가 수를 계산해 내면 인간이 바둑돌을 대신 놓아줘야 한다. 구글은 인공지능에 로봇이라는 몸체를 결합해 직접 임무를 수행하도록 할 계획이다.구글이 개발 중인 인공지능 로봇 팔. 크기와 모양이 제각각인 여러 물건을 집게 손으로 정확하게 들어 올릴 수 있도록 했다. /구글구글은 우선 로봇 팔 개발에 나서고 있다. 알파고가 인간이 둔 기보(棋譜)를 학습해 바둑을 익힌 것처럼, 로봇 팔도 학습을 거쳐 더 정교하게 움직이도록 개발하는 것이다. 물건을 집었을 때 집게에 정확하게 잡혔는지, 아니면 헛손질을 했는지 등을 데이터화해 시행착오를 줄여나가고 있다.일본 소니는 최근 100억엔(약 1130억원) 규모 펀드를 조성해 로봇·인공지능 분야에 투자하기로 했다. 히라이 가즈오 소니 최고경영자(CEO)는 \"애완견 로봇 아이보 사업을 재개하는 수준이 아니라, 로봇과 인공지능이 불러올 전자 산업의 새 영역에 발을 들이는 것\"이라고 말했다. 소니는 1999년 아이보를 내놔 인기를 끌었지만 경영난으로 2006년 사업을 접었다. 당시 삼성전자에 TV 세계 선두 자리를 내줬고 스마트폰 시장에선 중국 업체들에도 밀리며 어려움을 겪었다. 소니는 10년 만에 가정용 로봇 시장에 재진출하며 명예 회복을 노리고 있다.작년 12월 일본 도쿄에서 열린 로봇 전시회에서 도요타 직원이 도우미 로봇 ‘HSR’에 음성 명령을 내리고 있다(위 사진). 아래 사진은 현대차그룹 연구원이 웨어러블(착용형) 로봇을 입고 무거운 문짝을 들어 올리는 모습. /연합뉴스, 현대차그룹일본 도요타는 2020년 노인·장애인 도우미 로봇을 출시할 계획이다. 2012년 시제품으로 선보였던 'HSR' 로봇을 업그레이드해 양산 판매하겠다는 것이다. HSR은 60㎝짜리 팔로 사용자에게 물건을 집어줄 수 있다. 도요타는 올 초 로봇·인공지능 기술 개발을 위해 미국 실리콘밸리에 연구소를 세우고 미 국방고등연구계획국(DARPA) 출신 길 프랫 박사를 초대 소장으로 선임했다. 또 구글에서 로봇 제조사 보스턴다이내믹스와 샤프트를 인수하려고 막판 협상을 벌이고 있다.◇한국 기업들도 로봇 주목국내 대기업들도 서비스 로봇에 눈을 돌리고 있다. LG전자는 올해 연말부터 인천공항에서 안내 로봇 시범 운영에 들어갈 계획이다. 이용객의 질문을 알아듣고 한국어를 포함한 4국어로 안내할 수 있는 로봇이다. 정확한 답변을 위해 공항 개항 이후 접수한 승객들의 질문이나 불편 사항이 모두 로봇에 입력된다. 현대차그룹은 지난 5월 군사용, 산업용, 재활 치료용 등으로 이용 가능한 웨어러블(착용형) 로봇 시제품을 공개했다. 이 로봇은 산업 현장에서 사람 힘으로는 들 수 없는 무거운 물체를 쉽게 들게 해주고, 노약자나 장애인들이 계단을 오를 수 있도록 도와준다. 현대차는 여기에 쓰인 기술을 인공지능이나 자율주행 등과 결합해 차세대 운송 수단을 개발한다는 계획이다.한국의 로봇 청소기 기술은 이미 세계 최고 수준이다. 삼성전자와 LG전자는 올 초 스마트폰으로 집 안의 특정 공간을 터치하면 알아서 찾아가 청소하는 로봇 청소기를 공개했다. 청소기가 카메라를 통해 집 안 구조를 기억하고 움직이는 것이다. IT(정보 기술) 업계 관계자는 \"로봇을 새로운 성장 동력으로 삼으려고 점점 더 많은 글로벌 기업이 기술 경쟁에 가세할 것\"이라고 말했다. [채민기 기자 chaepline@chosun.com][조선닷컴 바로가기]- Copyrights ⓒ 조선일보 & chosun.com, 무단 전재 및 재배포 금지 -\n"
     ]
    }
   ],
   "source": [
    "comment, rank_idx, distance = zip(*result)\n",
    "idx = rank_idx[0]\n",
    "print(idx)\n",
    "print(article_series[idx].split(\"▶\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
