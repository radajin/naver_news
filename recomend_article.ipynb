{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import konlpy.tag \n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Article & Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_df(df):\n",
    "    return df.iloc[np.random.permutation(len(df))].reset_index(drop=True) \n",
    "\n",
    "def get_datas(date, comment=1):\n",
    "    article_df = pd.read_csv(\"./news/\" + date + \".csv\")\n",
    "    article_df = shuffle_df(article_df)\n",
    "    comment_df = pd.read_csv(\"./comment/comment\"+str(comment)+\".csv\")\n",
    "    return article_df, comment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Morpheme Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_document_morpheme(texts):\n",
    "    \"\"\"make documents list after morphological\"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    komoran = konlpy.tag.Komoran()\n",
    "    \n",
    "    for text in texts:\n",
    "        obj = line2obj(komoran.pos(text, flatten=False))\n",
    "        documents.append(obj)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "def line2obj(lines):\n",
    "    \"\"\" make keywords dictionary include only (NNP, NNG)\"\"\"\n",
    "    \n",
    "    obj = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        for keyword in line:\n",
    "            if len(keyword[0]) > 1 and ( keyword[1] == \"NNP\" or keyword[1] == \"NNG\" ) :\n",
    "                key = keyword[0]\n",
    "                if key in obj:\n",
    "                    obj[key] += 1\n",
    "                else:\n",
    "                    obj[key] = 1\n",
    "                    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_text_documents(documents):\n",
    "    \n",
    "    text_documents = []\n",
    "    \n",
    "    for document in documents:\n",
    "        \n",
    "        text_words = []\n",
    "        \n",
    "        for word, count in document.items():\n",
    "            text_words.extend([word] * count)\n",
    "            \n",
    "        text_document = \" \".join(text_words)\n",
    "        text_documents.append(text_document)\n",
    "        \n",
    "    return text_documents        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_euc_dist(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    dist = sp.linalg.norm(delta.toarray())\n",
    "    return dist\n",
    "\n",
    "def get_euc_dists(vectorized, num_docs):\n",
    "    \n",
    "    dists = []\n",
    "    \n",
    "    for idx in range(num_docs-1):\n",
    "        dist = calc_euc_dist(vectorized.getrow(num_docs-1), vectorized.getrow(idx))\n",
    "        dists.append((num_docs-1, idx, dist))\n",
    "        \n",
    "    return dists\n",
    "\n",
    "def sort_dists(dists):\n",
    "    result_list = []\n",
    "    for i, j, dist in sorted(dists, key=operator.itemgetter(2)):\n",
    "        result_list.append((i, j, dist))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1873, 110)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "article_df, comment_df = get_datas(\"2016-07-07\", 2)\n",
    "len(article_df), len(comment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 261 ms, total: 11 s\n",
      "Wall time: 5.31 s\n"
     ]
    }
   ],
   "source": [
    "# make documents\n",
    "comment_str = comment_df[\"content\"][:].str.cat(sep=' ')\n",
    "article_series = article_df[\"content\"][:100]\n",
    "article_series = article_series.append(pd.Series([comment_str])).reset_index(drop=True)\n",
    "%time article_documents = make_document_morpheme(article_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make gen documents\n",
    "gen_articles = gen_text_documents(article_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of input documents : 101\n",
      "the number of words : 6007\n"
     ]
    }
   ],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = HashingVectorizer()\n",
    "\n",
    "vectorized = vectorizer.fit_transform(gen_articles)\n",
    "num_docs, num_features =  vectorized.shape\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"the number of input documents : {}\".format(num_docs))\n",
    "print(\"the number of words : {}\".format(num_features))\n",
    "result_df = pd.DataFrame(vectorized.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Measurement Distance between comment and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 58, 1.3118112301585028),\n",
       " (100, 9, 1.3236153224507836),\n",
       " (100, 23, 1.3305864643786132),\n",
       " (100, 62, 1.3315614933374107),\n",
       " (100, 50, 1.3349254704047226),\n",
       " (100, 7, 1.3465324207978364),\n",
       " (100, 79, 1.3472167776139905),\n",
       " (100, 37, 1.3527655933910587),\n",
       " (100, 24, 1.3546201061158731),\n",
       " (100, 22, 1.3559338154407272)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = get_euc_dists(vectorized, num_docs)\n",
    "result = sort_dists(dists)\n",
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 하늘은 어째서 박정희를 다시 불러냈는가!!]\n",
      "\n",
      "아래 댓글에서 삿갓은 빅정희를 다시 불러낸 것은 하늘이라고 주장했다.\n",
      "그러면 하늘은 어째서 박정희를 다시 불러냈는가.\n",
      "\n",
      "이순신이나\n"
     ]
    }
   ],
   "source": [
    "# comment\n",
    "print(article_series[comment[0]][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recomend Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "국정원 대선개입 논란에 남북정상회담 회의록 유출은 이미 유죄  [CBS노컷뉴스 김효은 기자]박근혜 대통령(사진=청와대 제공)\"컴퓨터를 빼앗기면 국가정보원 심리전단 소속 직원들의 인터넷 게시글 등 대선 개입 활동 내용이 수사기관·언론 등에 공개될 수 있다는 점에 대한 두려움을 느껴 스스로 밖으로 나가지 않은 것이다\"법원이 지난 6일 국정원 여직원 감금 혐의로 기소된 민주통합당(현 더불어민주당) 이종걸 의원과 문병호·강기정·김현 전 의원에게 무죄를 선고하면서 밝힌 내용이다. 18대 대통령 선거를 일주일 앞둔 2012년 12월 11일 이 의원 등이 들이닥친 서울 역삼동 오피스텔 안에서는 국정원 심리전단 소속 여직원인 김모씨가 인터넷 게시판 등에서 여권에 유리한 정치 댓글을 달고 있었다.김씨는 노트북과 컴퓨터를 제출하라는 민주통합당 관계자와 경찰의 거듭된 요구에도 문을 걸어잠근 채 밖으로 나오지 않았다. 법원은 김씨가 나오지 않은 이유를 대선 개입이 알려질까봐 두려워했기 때문이라고 봤다.지난 2012년 12월 13일 민주통합당 문재인 후보 측에 악성댓글 유포 혐의를 받고 있는 국정원 여직원 김 모 씨가 역삼동 자신의 오피스텔에서 경찰의 증거자료를 압수를 지켜보고 있다. (사진=황진환 기자)국민이 직접 대통령을 뽑는 직선제 국가에서 국정원의 대선 개입은 명백한 관권선거다. 정보기관의 비호를 받은 정권은 정치적 정당성에 치명타를 입을 수밖에 없다.이 때문에 박근혜 대통령은 임기 초반부터 관권선거 시비에 시달려야만 했다. 원세훈 전 국정원장의 대선 개입 사건과 남북정상회담 대화록 유출 사건, 국정원 여직원 감금 사건의 당사자들이 줄줄이 법정에 섰다.가장 먼저 결론이 난 대화록 유출 사건을 제외하면 나머지 두 사건은 아직 사법부의 최종 판단을 기다리고 있다. 19대 대선이 치러지는 내년 12월까지 1년 6개월의 시간이 남아있다는 점을 감안할 때 박 대통령은 임기 말까지 관권선거 의혹에서 완전히 벗어나지는 못할 듯하다. 국정원 직원들에게 정치 댓글 작성을 지시한 혐의로 기소된 원세훈 전 원장의 파기환송심은 13차 공판기일을 거쳤음에도 결론이 나지 않은 상황이다.앞서 1심 재판부는 국정원의 정치 관여를 인정해 국정원법 위반을 유죄로 판단했지만, 공직선거법 위반은 무죄라고 밝혔다. 그러나 항소심 재판부는 선거법도 유죄라고 판단했다. 대법원은 상고심에서 유·무죄를 판단하지 않고 증거능력만을 문제 삼아 원심을 깨고 사건을 서울고법으로 돌려보냈다. 지난해 11월 서울고법 형사7부(김시철 부장판사) 심리로 파기환송심 첫 공판이 열렸지만, 반 년이 넘도록 최종 판단은 미뤄지고 있다. 일각에서는 재판부가 정치 일정을 고려해 의도적으로 재판을 지연시키고 있다는 의혹도 나오고 있다.파기환송심 결과에 따라서 관권선거 의혹은 의혹으로만 남을 수도 있고, 사실로 굳어질 수도 있다. 내년 12월 대선이라는 초대형 정치 이벤트를 앞두고 파기환송심은 어떤 방식으로 결론이 나든 엄청난 후폭풍을 일으킬 전망이다.한편, 지난 2014년 12월 새누리당 정문헌 전 의원은 남북정상회담 대화록을 유출한 혐의로 벌금 1000만원을 선고 받았다. 정 전 의원은 대선을 두 달 남겨둔 2012년 10월 청와대 통일비서관 재직 당시 열람했던 대화록 내용을 근거로 통일부 국정감사장에서 고 노무현 전 대통령이 서해북방한계선(NLL) 포기 의사를 밝혔다고 주장했다.대선 당시 선거를 총괄했던 새누리당 김무성 의원은 대선을 불과 닷새 앞둔 시점에서 부산 서면 유세장을 찾아 대화록 내용이라며 토씨 하나 틀리지 않고 정 전 의원이 주장한 내용을 그대로 따라 읊어 논란이 일었다. africa@cbs.co.kr저작권자 © CBS 노컷뉴스(www.nocutnews.co.kr) 무단전재 및 재배포 금지 \n"
     ]
    }
   ],
   "source": [
    "comment, rank_idx, distance = zip(*result)\n",
    "idx = rank_idx[0]\n",
    "print(idx)\n",
    "print(article_series[idx].split(\"▶\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
