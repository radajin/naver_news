{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using collaboration-filter (user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(date):\n",
    "    \n",
    "    article_df, comment_df = \"\", \"\"\n",
    "    \n",
    "    with open(\"./data/article_\" + date + \".plk\", 'rb') as file:\n",
    "        article_df = pickle.load(file)\n",
    "        # remove overlab\n",
    "        article_df = article_df[np.invert(article_df.duplicated(subset=\"newsid\"))]\n",
    "        \n",
    "    with open(\"./data/comment_\" + date + \".plk\", 'rb') as file:\n",
    "        comment_df = pickle.load(file)\n",
    "        \n",
    "    return article_df, comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = \"2016-06-01\"\n",
    "article_df, comment_df = get_data(date)\n",
    "len(article_df), len(comment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aritcle_user(article_df, comment_df, search_user_num=0, user_comment_num=0):\n",
    "\n",
    "    aid_columns = list(article_df[\"newsid\"])\n",
    "    user_id_list = comment_df[\"userIdNo\"].unique()\n",
    "    \n",
    "    # set search as user comment number\n",
    "    if search_user_num == 0:\n",
    "        \n",
    "        search_user_num = len(user_id_list)\n",
    "    \n",
    "    # make list that user_id and aid\n",
    "    user_list = []\n",
    "    \n",
    "    for idx, user_id in enumerate(user_id_list[:search_user_num]):\n",
    "        \n",
    "        if idx%5000 == 0:\n",
    "            print(idx, search_user_num)\n",
    "        \n",
    "        aid_list = list(comment_df[comment_df[\"userIdNo\"] == user_id][\"aid\"])\n",
    "        aid_list = [ int(aid) for aid in aid_list]\n",
    "        \n",
    "        tmp_dict = {\n",
    "            \"user_id\": user_id,\n",
    "            \"aid\": [ aid for aid in aid_list if aid in aid_columns ] \n",
    "        }\n",
    "\n",
    "        user_list.append(tmp_dict)\n",
    "    \n",
    "    # cut as user comment number\n",
    "    result_user_list = []\n",
    "    \n",
    "    if user_comment_num != 0:\n",
    "        \n",
    "        result_user_list = [ user for user in user_list if len(user[\"aid\"]) > user_comment_num]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        result_user_list = user_list\n",
    "    \n",
    "    return aid_columns, result_user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_aid_count(aid_column, user_aid):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for aid in user_aid:\n",
    "        \n",
    "        if aid_column == aid:\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "    return count \n",
    "\n",
    "def make_data_set(aid_columns, user_list):\n",
    "    \n",
    "    # make pandas dataframe into aid_columns and user_sample\n",
    "    df = pd.DataFrame(columns=aid_columns)\n",
    "    \n",
    "    for idx, user in enumerate(user_list):\n",
    "        \n",
    "        if idx%5000 == 0:\n",
    "            print(idx, len(user_list))\n",
    "        \n",
    "        user_sample = []\n",
    "        \n",
    "        for idx, aid_column in enumerate(aid_columns):\n",
    "\n",
    "            if aid_column in user[\"aid\"]:\n",
    "                \n",
    "                count = user_aid_count(aid_column, user[\"aid\"])\n",
    "                user_sample.append(count)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                user_sample.append(0)\n",
    "                \n",
    "        df.loc[len(df)] = user_sample\n",
    "    \n",
    "    # remove sum 0 colums from dataframe\n",
    "    remove_column_list = list(df.columns[df.sum(axis=0) == 0])\n",
    "    df.drop(remove_column_list, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aritcle_user(,,사용자수, 사용자댓글수)\n",
    "%time aid_columns, user_list = aritcle_user(article_df, comment_df, 0, 0)\n",
    "len(aid_columns), len(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time data_df = make_data_set(aid_columns, user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save\n",
    "date = \"2016-06-01\"\n",
    "with open(\"./data/data_df_\" + date + \".plk\", 'wb') as file:\n",
    "    pickle.dump(data_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load\n",
    "date = \"2016-06-01\"\n",
    "\n",
    "with open(\"./data/data_df_\" + date + \".plk\", 'rb') as file:\n",
    "    data_df = pickle.load(file)\n",
    "    print(len(data_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cosin_dist(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)  \n",
    "    dist = 1.0 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    return dist\n",
    "\n",
    "def get_dist_list(df, user_num=0):\n",
    "    \n",
    "    sample = df.loc[user_num]\n",
    "    \n",
    "    dists = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        dist = calc_cosin_dist(sample, row)\n",
    "        if dist < 0.00001:\n",
    "            dist = 0\n",
    "        dists.append((idx, dist))\n",
    "    \n",
    "    return dists\n",
    "    \n",
    "def sort_dists(dists, user_num=0):\n",
    "\n",
    "    if user_num == 0:\n",
    "        user_num = len(dists)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for i, dist in sorted(dists, key=operator.itemgetter(1)):\n",
    "        result_list.append((i, dist))\n",
    "    \n",
    "    result_list = result_list[1:user_num+1]\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dists(data_df):\n",
    "    dist_list = []\n",
    "    \n",
    "    for user in data_df.index:\n",
    "        dists = get_dist_list(data_df, user)\n",
    "        dists = sort_dists(dists, 3)\n",
    "        dist_list.append(dists)\n",
    "    return dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(data_df):\n",
    "    \n",
    "    dist_list = dists(data_df)\n",
    "    \n",
    "    predict_df = pd.DataFrame(columns=data_df.columns)\n",
    "    \n",
    "    for dist in dist_list:\n",
    "    \n",
    "        dist_index, dist_value = zip(*dist)\n",
    "        recomend_aids = data_df.loc[dist_index,:].mean(axis=0)\n",
    "        predict_df.loc[len(predict_df)] = recomend_aids\n",
    "        \n",
    "    return predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time predict_df = predict(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_test(data_df, predict_df):\n",
    "    \n",
    "    same_count = 0\n",
    "    \n",
    "    for idx1 in data_df.index:\n",
    "            \n",
    "        for idx2 in data_df.columns:\n",
    "            if (data_df.loc[idx1][idx2] == 0) and (predict_df.loc[idx1][idx2] == 0):\n",
    "                same_count += 1\n",
    "            elif (data_df.loc[idx1][idx2] > 0) and (predict_df.loc[idx1][idx2] > 0):\n",
    "                same_count += 1\n",
    "                \n",
    "    return same_count / (len(data_df.index)*len(data_df.columns))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time compare_test(data_df, predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recomend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recomend(data_df, predict_df, idx=0):\n",
    "    \n",
    "    predict_datas = predict_df.loc[idx]\n",
    "    user_datas = data_df.loc[idx]\n",
    "    \n",
    "    predict_datas = predict_datas[predict_datas > 0]\n",
    "    user_datas = user_datas[user_datas > 0]\n",
    "    \n",
    "    # remove already write comment aid\n",
    "    for predict_idx in predict_datas.index:\n",
    "        for user_idx in user_datas.index:\n",
    "            if predict_idx == user_idx:\n",
    "                predict_datas = predict_datas.drop(user_idx)\n",
    "    \n",
    "    # sorting\n",
    "    predict_datas = predict_datas.sort_values(ascending=False)\n",
    "\n",
    "    return predict_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_datas = recomend(data_df, predict_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recomend(predict_df):\n",
    "    predict_datas = predict_df.loc[0]\n",
    "    user_datas = predict_df.loc[1]\n",
    "    \n",
    "    predict_datas = predict_datas[predict_datas > 0]\n",
    "    user_datas = user_datas[user_datas > 0]\n",
    "    \n",
    "    # remove already write comment aid\n",
    "    for predict_idx in predict_datas.index:\n",
    "        for user_idx in user_datas.index:\n",
    "            if predict_idx == user_idx:\n",
    "                predict_datas = predict_datas.drop(user_idx)\n",
    "    \n",
    "    # sorting\n",
    "    predict_datas = predict_datas.sort_values(ascending=False)\n",
    "    \n",
    "    return predict_datas, user_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_datas, user_datas = recomend(predict_df)\n",
    "len(predict_datas), len(user_datas), predict_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### content-based => factor => Matirix factorization\n",
    "- 몇개의 factor를 조합해서 (예: 해피엔딩, 배드엔딩 / 코메디, 새드 ) 몇가지 factor로 데이터를 추론함\n",
    "- 사용자 10억, 영화1억, 요소 4개 -> 사용자(matrix : 10억*4) * 영화(matrix : 4*1억) = 추천지수\n",
    "\n",
    "$Q^TP = R$ 최소화 하는 Q,P를 찾아야함\n",
    "\n",
    "### collaboration-filter\n",
    "- User-Based : user를 기준으로 비슷한 user를 찾아서 채움\n",
    "- Item-Based : item을 기준으로 비슷한 item을 찾아서 채움\n",
    "\n",
    "빈칸 메우기 - Imputation\n",
    "실제 데이터 - 타이타닉 데이터\n",
    "\n",
    "pd - groupby transform를 이용해서 비슷한 데이터에 대한 빈칸을 채움\n",
    "\n",
    "- User-Based\n",
    "similarity matrix\n",
    "  | A | B | C\n",
    "\\----------------\n",
    " A| 1   -   2\n",
    "\\----------------\n",
    " B| 0   2   -\n",
    "\\----------------\n",
    " C| 2   3   4\n",
    "\n",
    "사용자에 대해서 하고 sorting이나 ranking을 한다.\n",
    "한 10명정도에 대한 평균을 낸다.(가중치를 줘서 weighted average를 준다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec - 단어 예측기\n",
    "\n",
    "- distributed word vectorigation\n",
    "\n",
    "gre문제 \n",
    "\n",
    "\n",
    "- 확률적으로 단어를 나오게 마코프 체인을 많이 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
