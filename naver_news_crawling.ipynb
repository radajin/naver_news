{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    \"100\":950203, # 정치\n",
    "    \"101\":949986, # 경제\n",
    "    \"102\":949987, # 사회\n",
    "    \"103\":949988, # 생활/문화\n",
    "    \"104\":949990, # 세계\n",
    "    \"105\":949984, # IT/과학\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crawling Last Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_page(category, date):\n",
    "    compnentId = category_dict[str(category)]\n",
    "    url = \"http://news.naver.com/main/mainNews.nhn?componentId=\" + str(compnentId) + \"&date=\" + date + \" 00:00:00&page=100\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()[\"pagerInfo\"][\"page\"]\n",
    "    \n",
    "# last_page(100, \"2016-06-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crawling Content, Comment, LikeIt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using json\n",
    "def get_likeit(aid, oid):    \n",
    "    url = \"http://news.like.naver.com/likeIt/likeItContent.jsonp?_callback=window.__jindo2_callback._7105&serviceId=NEWS&displayId=NEWS&contentsId=ne_\" + str(oid) + \"_\" + str(aid) + \"&lang=ko&viewType=recommend\"\n",
    "    response = requests.get(url)\n",
    "    return response.text.split('likeItCount\":')[1].split(\",\")[0]\n",
    "    \n",
    "# using bs4\n",
    "def get_content(path):\n",
    "    \n",
    "    response = requests.get(path)\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    if len(dom.select(\"#articleTitleCommentCount .lo_txt\")) == 0:\n",
    "        return 0, 0, \"-\"\n",
    "    \n",
    "    comment = dom.select_one(\"#articleTitleCommentCount .lo_txt\").text\n",
    "    content = dom.select_one(\"#articleBodyContents\").text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "    aid = path.split(\"aid=\")[1]\n",
    "    oid = path.split(\"oid=\")[1].split(\"&\")[0]\n",
    "    likeit = get_likeit(aid, oid)\n",
    "    \n",
    "    return comment, likeit, content\n",
    "\n",
    "# url = \"http://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=003&aid=0007327243\"\n",
    "# content_data = get_content(url)\n",
    "# content_data[0], content_data[1], len(content_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crawling 1 category, 1 day, 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_page_df(category, date, page):\n",
    "    \"\"\" excute time about 5 ~ 6 sec \"\"\"\n",
    "\n",
    "    url = \"http://news.naver.com/main/mainNews.nhn?componentId=\" + str(category_dict[str(category)]) + \"&date=\" + date + \" 00:00:00&page=\" + str(page)\n",
    "    response = requests.get(url)\n",
    "    article_list = response.json()[\"itemList\"]\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=[\"newsid\", \"oid\", \"newspaper\", \"title\", \"link\", \"comment\", \"likeit\", \"content\", \"date\", \"category\"])\n",
    "\n",
    "    for article in article_list:\n",
    "        link = \"http://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=\" + str(category) + \"&oid=\" + article[\"officeId\"] + \"&aid=\" + article[\"articleId\"]        \n",
    "        comment, likeit, content = get_content(link)\n",
    "        \n",
    "        tmp_dict = {\n",
    "            \"newsid\": article[\"articleId\"],\n",
    "            \"oid\": article[\"officeId\"],\n",
    "            \"newspaper\": article[\"officeName\"],\n",
    "            \"title\": article[\"title\"],\n",
    "            \"link\": link,\n",
    "            \"comment\": comment,\n",
    "            \"likeit\": likeit,\n",
    "            \"content\": content.split(\"▶\")[0],\n",
    "            \"date\": date,\n",
    "            \"category\": str(category-100),\n",
    "        }\n",
    "        \n",
    "        result_df.loc[len(result_df)] = tmp_dict\n",
    "        \n",
    "    return result_df\n",
    "\n",
    "# df = one_page_df(105, \"2016-07-07\", 2)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 category, 1 day, all page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_day_df(category, date):\n",
    "    \"\"\" excute time about 60 sec / 10 page \"\"\"\n",
    "    \n",
    "    last_page_number = int(last_page(category, date))\n",
    "    \n",
    "    print(\"last page : {} / {} / {}\".format(last_page_number, category, date))\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for page in range(1, last_page_number + 1):\n",
    "        df = one_page_df(category, date, page)\n",
    "        df_list.append(df)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    return pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "# day_df = one_day_df(100, \"2016-07-06\")\n",
    "# len(day_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Daily Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def day_news(date):\n",
    "    df_list = []\n",
    "    for category in range(100, 106):\n",
    "        day_df = one_day_df(category, date)\n",
    "        df_list.append(day_df)\n",
    "    \n",
    "    return pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "def get_monthly_article(month, startday, lastday):\n",
    "    for day in range(startday, lastday+1):\n",
    "        month = \"0\" + str(month) if 10 > month else str(month)\n",
    "        day = \"0\" + str(day) if 10 > day else str(day)\n",
    "        date = \"2016-\" + month + \"-\" + day\n",
    "        df = day_news(date)\n",
    "        df.to_csv(\"./news/\" + date + \".csv\", index=False, encoding=\"utf-8\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
